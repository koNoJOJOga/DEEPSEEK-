{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable VLLM's progress bars\n",
    "logging.getLogger(\"vllm\").setLevel(logging.WARNING)\n",
    "\n",
    "# Constants from training script\n",
    "R1_STYLE_SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
    "with the answer. The reasoning process and answer are enclosed within   and\n",
    "  tags, respectively, i.e.,  reasoning process here \n",
    " answer here .\"\"\"\n",
    "\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"The answer must be a single integer.\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    try:\n",
    "        answer = text.split(\"\")[-1].split(\"\")[0].strip()\n",
    "        return answer\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    try:\n",
    "        return text.split(\"####\")[1].strip()\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model_path: str,\n",
    "    batch_size: int = 4,\n",
    "    num_samples: int = None,\n",
    "    save_results: bool = True,\n",
    "    gpu_memory_utilization: float = 0.3,\n",
    ") -> Dict:\n",
    "    print(\"Initializing evaluation...\")\n",
    "\n",
    "    # Initialize VLLM with progress indicator\n",
    "    with tqdm(total=2, desc=\"Loading model components\") as pbar:\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            dtype=\"half\",\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "            max_model_len=768,\n",
    "            device=\"cuda:0\",\n",
    "            enable_chunked_prefill=True,\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            model_max_length=768,\n",
    "            padding_side='right',\n",
    "            truncation_side='right'\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Set up sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=512,  # Matching max_completion_length from training\n",
    "        stop_token_ids=[tokenizer.eos_token_id],\n",
    "    )\n",
    "\n",
    "    # Load test dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(num_samples))\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Loaded {total_samples} samples\")\n",
    "\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(\n",
    "        total=total_samples,\n",
    "        desc=\"Processing samples\",\n",
    "        unit=\"examples\",\n",
    "        dynamic_ncols=True,\n",
    "    )\n",
    "\n",
    "    progress_bar.set_postfix({\n",
    "        'acc': '0.00%',\n",
    "        'correct': '0',\n",
    "    })\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_data = dataset[i:i + batch_size]\n",
    "        current_batch_size = len(batch_data['question'])\n",
    "\n",
    "        # Prepare prompts using same format as training\n",
    "        prompts = [\n",
    "            [\n",
    "                {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
    "                {'role': 'user', 'content': \"What is 2+2?\"},\n",
    "                {'role': 'assistant', 'content': \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n4\"},\n",
    "                {'role': 'user', 'content': q.strip()}\n",
    "            ] for q in batch_data['question']\n",
    "        ]\n",
    "\n",
    "        # Convert to chat format\n",
    "        formatted_prompts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                p,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            for p in prompts\n",
    "        ]\n",
    "\n",
    "        # Generate responses\n",
    "        outputs = llm.generate(\n",
    "            formatted_prompts,\n",
    "            sampling_params,\n",
    "        )\n",
    "\n",
    "        # Process responses\n",
    "        for j, output in enumerate(outputs):\n",
    "            response = output.outputs[0].text\n",
    "\n",
    "            # Extract answers\n",
    "            generated_answer = extract_xml_answer(response)\n",
    "            true_answer = extract_hash_answer(batch_data['answer'][j])\n",
    "\n",
    "            # Store result\n",
    "            result = {\n",
    "                'question': batch_data['question'][j],\n",
    "                'true_answer': true_answer,\n",
    "                'generated_answer': generated_answer,\n",
    "                'full_response': response,\n",
    "                'correct': generated_answer == true_answer\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Update metrics\n",
    "            if generated_answer == true_answer:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        # Update progress\n",
    "        progress_bar.update(current_batch_size)\n",
    "        progress_bar.set_postfix({\n",
    "            'acc': f'{(correct/total)*100:.2f}%',\n",
    "            'correct': f'{correct}/{total}',\n",
    "        })\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'model_path': model_path,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    if save_results:\n",
    "        save_path = f\"gsm8k_eval_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'metrics': metrics,\n",
    "                'results': results\n",
    "            }, f, indent=2)\n",
    "        print(f\"\\nResults saved to {save_path}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting GSM8K evaluation...\")\n",
    "checkpoint_path = \"outputs/Qwen2.5-0.5B-Instruct-GRPO/checkpoint-latest\"  # Update path as needed\n",
    "\n",
    "metrics = evaluate_model(\n",
    "    model_path=checkpoint_path,\n",
    "    batch_size=4,\n",
    "    num_samples=None,\n",
    "    save_results=True,\n",
    "    gpu_memory_utilization=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
    "print(f\"Correct: {metrics['correct']}/{metrics['total']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
