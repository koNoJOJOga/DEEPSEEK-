{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl.trainer import GRPOConfig, GRPOTrainer\n",
    "\n",
    "\n",
    "R1_STYLE_SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
    "with the answer. The reasoning process and answer are enclosed within   and\n",
    "  tags, respectively, i.e.,  reasoning process here \n",
    " answer here .\"\"\"\n",
    "\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"The answer must be a single integer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset_name, split=\"train\", chunk_size=1000) -> Dataset:\n",
    "    dataset = load_dataset(dataset_name, 'main')[split]\n",
    "\n",
    "    def extract_hash_answer(text: str) -> str | None:\n",
    "        try:\n",
    "            return text.split(\"####\")[1].strip()\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "    def process_batch(batch):\n",
    "        prompts = [[\n",
    "            {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
    "            {'role': 'user', 'content': \"What is 2+2?\"},\n",
    "            {'role': 'assistant', 'content': \"To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.\\n4\"},\n",
    "            {'role': 'user', 'content': q.strip()}\n",
    "        ] for q in batch['question']]\n",
    "\n",
    "        return {\n",
    "            'prompt': prompts,\n",
    "            'answer': [extract_hash_answer(a) for a in batch['answer']]\n",
    "        }\n",
    "\n",
    "    return dataset.map(process_batch, batched=True, batch_size=chunk_size)\n",
    "\n",
    "dataset_name = 'openai/gsm8k'\n",
    "dataset = preprocess_dataset(dataset_name, chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    try:\n",
    "        answer = text.split(\"\")[-1].split(\"\")[0].strip()\n",
    "        return answer\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "\n",
    "# reward functions\n",
    "# VALID_FORMAT = re.compile(r\"(?:(?!?reasoning>|?answer>).)*\\n(?:(?!?reasoning>|?answer>).)*\")\n",
    "\n",
    "# def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "#     \"\"\"Reward function that checks if the completion has the correct format.\"\"\"\n",
    "#     responses = [completion[0][\"content\"] for completion in completions]\n",
    "#     matches = [bool(VALID_FORMAT.fullmatch(r.strip())) for r in responses]\n",
    "#     return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has the correct format.\"\"\"\n",
    "    pattern = r\"^(?:(?!).)*\\n(?:(?!).)*$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [bool(re.match(pattern, r)) for r in responses]\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the answer is correct.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print(f\"Question: {prompts[0][-1]['content']}\\nAnswer: {answer[0]}\\nResponse: {responses[0]}\\nExtracted: {extracted_responses[0]}\")\n",
    "    print(''.join('✅' if r == a else '❌' for r, a in zip(extracted_responses, answer)))\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "output_dir = f\"outputs/{model_name.split('/')[-1]}-GRPO\"\n",
    "run_name = f\"{model_name.split('/')[-1]}-{dataset_name.split('/')[-1]}\"\n",
    "\n",
    "\n",
    "# Set memory-related environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "max_prompt_length=256\n",
    "max_completion_length=512\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=1e-5,\n",
    "    beta=0.005, # divergence coefficient – how much the policy is allowed to deviate from the reference model. higher value – more conservative updates. Default is 0.04\n",
    "    optim=\"adamw_8bit\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_generations=4,  # group size\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=True,\n",
    "    vllm_init_kwargs={\n",
    "        \"device\": \"cuda:0\",\n",
    "        \"gpu_memory_utilization\": 0.3,\n",
    "        \"max_model_len\": max_prompt_length + max_completion_length,\n",
    "        \"dtype\": \"half\",\n",
    "        # \"enable_chunked_prefill\": True,\n",
    "        # \"max_num_batched_tokens\": 2048,\n",
    "    },\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    logit_computation_mini_batch_size=1,\n",
    "    enable_profiling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\", # T4 is not supported\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=training_args.max_completion_length,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        format_reward_func,\n",
    "        correctness_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
